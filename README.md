# analysis-and-design

# README
ABCU is a hypothetical university, where the Computer Science academic advisors needed a new system for generating the course catalog. In addition, they wanted the ability to look up course prerequisites easily. I was tasked with determining an appropriate data structure based on Big O time and memory complexity analyses. Each time the program is opened, it will read, parse, and validate information from a CSV file containing courses, descriptions, and prerequisites. Each line in this file is to contain all relevant information for a single course.
Each course must contain at least two elements: courseNumber (i.e. MATH 100) and a name (i.e. Algebra). If a course does not contain these components, the program aborts loading courses into the data structure. A future revision might include the ability to alert the user when/if this happens and prompt a response to continue or abort.
If a course contains prerequisites, each prerequisite must also be offered by the university to be valid.
I wrote a document (ProjectOne.docx) evaluating three different data structures, vectors, hashtables, and binary search trees. This document also contains pseudocode that would be a useful guide to write the application with any of these three structures.

The remaining files are C++ classes and header files to implement a binary search tree for this purpose. Even though typical universities logistically can only offer about 500 unique classes for a given program, a binary search tree offers the best combined runtime and memory complexity.

This exercise gave me an opportunity to explore data structures outside of typical vectors and arrays that most frequently are employed. By familiarizing myself with these data structures, I now have confidence in knowing which structure to choose and what data attributes to look for in making this determination. I made a couple of mistakes in my pseudocode based on assumptions, but I left these mistakes in the ProjectOne.docx so I can show my progress. Reading from a file, parsing lines, using streams to separate input, and loading this data into a class structure gave me the largest challenge when writing the actual code. While debugging, I found that I needed to import a couple more libraries to allow my streams to work properly. Once I did this, things worked quite well.

Once I finished writing the base code, I went back to refactor and clean up, where I found some opportunities to make things more user friendly and eliminate clumsy errors from crashing the program. In doing so, I made two rather simple mistakes that could have easily been missed. First, I forgot to clear the input stream in cin when a user chooses to select their own file path. Second, I forgot to test this functionality! Since these components were working perfectly prior to making an adjustment, I carelessly moved along without testing! Fortunately, this was brought to my attention and I was able to fix it with a single line of code!

Prior to working on this project, I would have used a vector and utilized a linear search in most scenarios like this one. I wrote code for all three data structures along with a CPU clock tick counter to measure time complexity given semi-similar data sets. Based on the way the data was organized at input (i.e. randomized or sorted), each data structure performed differently. Using a binary search tree, it is critical to either randomize the data prior to input or choose a different structure better suited for that scenario.

Outside of creating solid evidence to the benefits of certain algorithmic data structures, this project provided me with more practice to keep my code organized effectively through the use of classes, methods, and functions. Referencing and dereferencing pointers was an area with C++ I had been struggling to use properly until now. I can happily say that my comfort and confidence with C++ has skyrocketed!
